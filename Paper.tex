\documentclass[a4paper,usenames,dvipsnames,11pt]{article}
%\usepackage{jheppub}
\usepackage{cite}
\usepackage{tabularx,booktabs,multirow}
%\usepackage[toc,page]{appendix}
%\usepackage{authblk}
%\usepackage{multicol}
%\usepackage[T1]{fontenc}
%\usepackage[english]{babel}
%\usepackage{listings}
%\usepackage[pdftex]{graphicx}
\usepackage[makeroom]{cancel}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{bbold}
\usepackage{booktabs, cellspace, hhline}
\usepackage{multirow}
\usepackage{makecell}
%\setlength\cellspacetoplimit{4pt}
%\setlength\cellspacebottomlimit{4pt}
\usepackage{graphicx}

%\bibliographystyle{spphys}
\usepackage{hyperref}
%\usepackage{dsfont}
%\usepackage[dvipsnames]{xcolor}
%\usepackage{fancyvrb}
%%\usepackage{textcomp}
%\usepackage{subcaption}
%\usepackage{slashed}
% \usepackage{graphicx}
\usepackage{xcolor}
% \usepackage{calrsfs}
%\renewcommand*{\arraystretch}{1}
\usepackage{setspace}
%\setstretch{1.2}
	\addtolength{\oddsidemargin}{-.9in}
	\addtolength{\evensidemargin}{-.9in}
	\addtolength{\textwidth}{1.6in}
	\addtolength{\topmargin}{-0.1in}
	\addtolength{\textheight}{0.6in}
%\usepackage{amssymb}
%\usepackage{pgf}
%\usepackage{tikz}
%\usepackage{braket}
%\usepackage{multicol}
%\usepackage{doi}
%\usepackage{tikz}
%\usepackage{xparse}
%\usepackage{empheq}
%\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
%\usepackage{wasysym}
%\newcommand*{\vpointer}{\vcenter{\hbox{\scalebox{2}{\Huge\pointer}}}}
\usepackage{colortbl}
\setcounter{MaxMatrixCols}{15}

\newcommand{\Tr}{\textrm{Tr}}
\newcommand{\len}{\textrm{len}}
\newcommand{\LC}{\textrm{LC}}
\newcommand{\NLC}{\textrm{NLC}}
\newcommand{\FC}{\textrm{FC}}
\newcommand{\RF}[1]{{\color{red} #1}}
\newcommand{\TV}[1]{{\textbf{\color{blue} #1} }}
\newcommand{\pmt}{$\pm$ }
\usepackage{authblk}
\usepackage{mleftright}

\usepackage[mathscr]{euscript}
\newcommand{\sA}{\mathscr{A}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sS}{\mathscr{S}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sN}{\mathscr{N}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sU}{\mathscr{U}}
\newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}}
\newcommand{\sI}{\mathscr{I}}
\newcommand{\sJ}{\mathscr{J}}
\newcommand{\sAt}{\tilde{\mathscr{A}}}
\newcommand{\sBt}{\tilde{\mathscr{B}}}
\newcommand{\sCt}{\tilde{\mathscr{C}}}
\newcommand{\sDt}{\tilde{\mathscr{D}}}
\newcommand{\sEt}{\tilde{\mathscr{E}}}
\newcommand{\sFt}{\tilde{\mathscr{F}}}
\newcommand{\sSt}{\tilde{\mathscr{S}}}
\newcommand{\sPt}{\tilde{\mathscr{P}}}
\newcommand{\sQt}{\tilde{\mathscr{Q}}}
\newcommand{\sRt}{\tilde{\mathscr{R}}}
\newcommand{\sNt}{\tilde{\mathscr{N}}}
\newcommand{\sMt}{\tilde{\mathscr{M}}}
\newcommand{\sUt}{\tilde{\mathscr{U}}}
\newcommand{\sVt}{\tilde{\mathscr{V}}}
\newcommand{\sWt}{\tilde{\mathscr{W}}}
\newcommand{\sIt}{\tilde{\mathscr{I}}}
\newcommand{\sJt}{\tilde{\mathscr{J}}}

\newcommand{\U}{\text{U}}
\newcommand{\NC}{N_{\text{\tiny C}}}
\newcommand{\SU}{\text{SU}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\dd}{\text{d}}

\newcommand{\MG}{\textsc{MadGraph$\_$aMC@NLO~}}

\newcommand{\AmpliCol}{\textsc{AmpliCol~}}

\newcommand{\gen}{gen2$\to$3}

\renewcommand{\arraystretch}{1.3}


\begin{document}


\title{\textbf{Reducing the scaling complexity for \\ high-multiplcity event generation with \AmpliCol}}

\date{}
\author{
Rikkert Frederix$^{1\,}$\footnote{E-mail:  \texttt{rikkert.frederix@fysik.lu.se}},
Timea Vitos$^{2,3}$\footnote{E-mail:  \texttt{timea.vitos@physics.uu.se}},\\
{\small\it $^{1}$ Department of Physics, Lund University,} 
\\%
{\small\it S\"olvegatan 14A, SE-223 62, Lund, Sweden}\\
{\small\it $^{2}$ Department of Physics and Astronomy, Uppsala University,} 
\\%
{\small\it Box  516,  751 20, Uppsala, Sweden  }\\
{\small\it $^{3}$ Institute for Theoretical Physics, ELTE  E\"otv\"os Lor\'and  University, } 
\\%
{\small\it P\'azm\'any  P\'eter  s\'et\'any  1/A,  H-1117  Budapest,  Hungary}\\
}
\maketitle




\begin{abstract}\noindent
The scaling of event generation with particle number is a challenge in particle physics. In this work, we target the efficient generation of realistic LHC processes with increasing number of jets in the final state. The main idea relies on a previously developed two-step approach, in which unweighted events are generated at leading-colour accuracy, and then in a second step reweighted to full-colour accuracy, thus remaining efficient in the integration step, but capturing the full-colour accuracy in the hard scattering process. We compare the computation time for four benchmark LHC processes: multi-jet, $t\bar{t}$ plus jets, $ZZ$ plus jets and Drell-Yan plus jets. The results show that the combined timing of generating a fixed number of unweighted events at full-colour accuracy is moderate and scales exponentially with the particle multiplicities we consider, vastly outsourcing the factorial growth in  conventional approaches.
\end{abstract}
\thispagestyle{empty}
\vfill


\newpage

\begingroup
\hypersetup{linkcolor=black}
\tableofcontents
\endgroup


\section{Introduction}

Accurate simulations of high‑energy collisions are indispensable for both precision measurements and searches for new physics at the LHC~\cite{Campbell:2022qmc}. As experimental analyses increasingly rely on multi‑jet final states and complex event topologies, the demands placed on matrix‑element generators continue to grow. Yet the computational cost of evaluating QCD amplitudes with many external partons remains a fundamental obstacle~\cite{HEPSoftwareFoundation:2017ggl}, even at leading-order (LO) accuracy: the colour algebra proliferates, the integrand becomes sharply structured, and the phase‑space integration slows to a crawl. Even with decades of progress---recursive amplitude construction~\cite{Berends:1987me,Caravaglios:1995cd,Caravaglios:1998yr,Draggiotis:1998gr,Britto:2004ap,Duhr:2006iq}, importance sampling~\cite{Lepage:1977sw,Lepage:2020tgj,vanHameren:2007pt,Kleiss:1994qy,Ohl:1998jn}, improved phase‑space mappings~\cite{Papadopoulos:2000tt,Krauss:2001iv,Maltoni:2002qb,Kilian:2007gr,Gleisberg:2008fv,Alwall:2014hca,Mattelaer:2021xdr,Bothmann:2023siu,Frederix:2024uvy,Sherpa:2024mfk}, stochastic sampling of discrete quantum numbers~\cite{Caravaglios:1998yr,Draggiotis:1998gr,Papadopoulos:2000tt,Mangano:2002ea,Gleisberg:2008fv}, utilising hardware acceleration~\cite{Kanzaki:2010ym,Hagiwara:2010oca,Hagiwara:2013oka,Bothmann:2021nch,Carrazza:2021gpx,Bothmann:2023gew,Cruz-Martinez:2025kwa,Valassi:2021ljk,Valassi:2022dkc,Valassi:2023yud,Hageboeck:2023blb,Valassi:2025xfn}, and machine-learning substitudes~\cite{Bendavid:2017zhk,Klimek:2018mza,Chen:2020nfb,Gao:2020vdv,Bothmann:2020ywa,Gao:2020zvv,Winterhalder:2021ngy,Butter:2022rso,Heimel:2022wyj,Heimel:2023ngj,Heimel:2024wph,Bothmann:2025lwg,Janssen:2025zke,Beccatini:2025tpk}---the scaling of the matrix‑element evaluation still limits the multiplicities that can be reached in practice.

A promising strategy, explored in a recent work~\cite{Frederix:2024uvy}, is to decouple integration efficiency from colour accuracy. Leading‑colour (LC) amplitudes capture the dominant kinematic structure of QCD matrix elements while avoiding the factorial growth of the full‑colour (FC) expansion. This observation motivates a two‑step workflow: generate unweighted events using the LC integrand, and subsequently restore FC accuracy by reweighting each event with the exact colour‑summed matrix element. In earlier work, this approach was shown to yield substantial gains for pure‑QCD processes, with LC amplitudes providing both a tractable integrand and a reweight factor that remains close to unity across the entire phase space.

In this paper we extend this strategy to general Standard Model processes and present \AmpliCol, a standalone implementation of the two‑step LC$\to$FC event‑generation framework. The extension to realistic LHC processes introduces new challenges---notably the treatment of colour‑singlet particles and multiple quark lines, which break the one‑to‑one correspondence between colour orderings and efficient phase‑space mappings defined in Ref.~\cite{Frederix:2024uvy}. We address these issues through a multi‑channel setup. With these ingredients, AmpliCol can generate LC unweighted samples efficiently for a broad class of processes and subsequently reweight them to FC accuracy with high secondary‑unweighting efficiency.

To quantify the performance of this approach, we study four representative LHC processes---multi‑jet production, $t\bar{t}+\textrm{jets}$, $ZZ+\textrm{jets}$, and Drell–Yan+jets---across increasing jet multiplicities. For each process we measure the wall‑time required to obtain $10^5$ unweighted FC events on a single CPU core. The results show a clear exponential scaling with multiplicity, with bases around five for all processes considered. This behaviour stands in sharp contrast to the factorial growth of conventional matrix-element generators and demonstrates that the LC integration step dominates the runtime up to moderately high multiplicities. We also analyse the scaling of the reweighting step, which grows faster than the LC integration and is expected to become the limiting factor at very high multiplicities. 

The remainder of the paper is organised as follows. Section 2 summarises the two‑step methodology and describes the extensions required for processes with colour singlets. Section 3 presents timing benchmarks and scaling fits for the selected processes. We conclude in section 4 and outline future improvements and prospects for integration with existing event‑generation frameworks. Appendix A provides additional diagnostics on the secondary unweighting efficiency and effective sample size.

%The current procedure for precise predictions and measurements of particle physics collisions relies heavily on computational simulations. Although the general-purpose collision simulators have been around for 20-30 years~\cite{Sjostrand:2014zea,Bahr:2008pv,Gleisberg:2008ta}, there have been numerous developments the past decades targeting various aspects of the computations, both in the width of the targeted processes and in their computational efficiencies. One still standing problem for event generators is the multiplicity handling: the computations are too slow or not even feasible when the final state multiplicity (often in terms of jets) in the collisions in the hard scattering reaches a (mostly algorithm-dependent) limit. The alternative has long been the usage of parton showers for the generation of multiple jets, with the obvious limitation of the range of validity to the soft and collision regions.

%The main computational challenge for these multi-jet events is due to the complex structure of quantum chromodynamics (QCD). The integrand in the computation of the cross section grows factorially with the external particle number \RF{strictly speaking not true: \cite{Bolinder:2025gbj}}, resulting in a highly inefficient integration due to the increasing integrand evaluation time. In conventional tools, this results in a break-down of the execution feasibility of the computation at some multiplicity numbers. Many works have contributed to improvements in this area, including the improvement of the phase-space integration~\cite{Lepage:1977sw,Lepage:2020tgj,vanHameren:2007pt,Kleiss:1994qy,Ohl:1998jn,Maltoni:2002qb,Papadopoulos:2000tt,Krauss:2001iv,Kilian:2007gr,Gleisberg:2008fv,Alwall:2014hca,Sherpa:2019gpd}, sampling of colour and helicity configurations~\cite{Berends:1987me,Caravaglios:1995cd,Caravaglios:1998yr,Draggiotis:1998gr,Mangano:2002ea,Duhr:2006iq,Gleisberg:2008fv,Mattelaer:2021xdr}, and recursion relations instead of conventional diagram-based computation of the matrix-elements~\cite{Berends:1987me,Britto:2004ap}.  Another approach is the reduction of the integrand by introducing a truncation in the colour expansion, either at leading-colour (LC), leading to a linear scaling in the number of terms with the external multiplicity, or at the next-to-leading colour (NLC) order yielding a polynomial scaling \cite{Frederix:2021wdv,Badger:2012pg}. There have been works targeting the efficient event generation of high-multiplicity events, including \textsc{Comix}~\cite{Gleisberg:2008fv} and \textsc{Pepper}~\cite{Bothmann:2023gew}. 

%One possible solution to the problem of increasing complexity in event generation was presented in our previous work Ref.~\cite{Frederix:2024uvy}. We suggest there a two-step approach for generating events at leading-order (tree-level) in the Standard Model coupling expansion. The first step constitutes a conventional phase-space integration, in which the amplitudes are computed with off-shell recursion relations~\cite{Berends:1987me,Britto:2004ap}, and the precision in the colour expansion is truncated in the first order, the LC approximation. This truncation results in a much more efficient evaluation of the matrix-elements, as the factorial complexity is reduced to a exponential complexity. In this manner, unweighted LC events are generated, covering all of the targeted phase-space, following a LC integrand in the importance sampling. In the second step, these unweighted events are passed through a reweighting module, upon which the full-colour (FC) matrix-elements are evaluated for each event, and with a secondary unweighting, the new set of FC events are obtained. 


%In this work, we present results for hadronic collisions, by properly dealing with partonic sub-processes in an efficient manner in the standalone, Fortran-based computer code which we dub \AmpliCol. We present results for four benchmark processes and compare the scaling behaviours for increasing jet multiplicity. The paper is structured in the following way: in Section~\ref{sec:method} we present the methodology by first briefly summarising the two-step event generation and discussing the phase-space integration. In Section~\ref{sec:results} we present our findings, and finally summarise and present an outlook in Section~\ref{sec:outlook}.



\section{The \AmpliCol approach}\label{sec:method}

The methodology adopted in the \AmpliCol program follows closely that presented in Ref.~\cite{Frederix:2024uvy} and is based on a two-step approach for event generation. In following we present a short summary of its main points. We refer the interested reader to the mentioned reference for further details of the method.

\subsection{Overview of the two-step event generation}

The main hurdle in the computation of tree-level cross sections is the highly complex and peaked matrix element $|\M^2|$ in the integrand,
\begin{align}\label{eq.LC}
\sigma_{\rm FC}\simeq \int |\M|^2\dd\Phi = \int \sum_{i,j} \A_i C_{ij}\A^*_j \dd\Phi,
\end{align}
where we have suppressed the parton density functions, flux factor and additional factors coming from colour and helicity averages. The integral can be rewritten as a double sum over colour-ordered amplitudes $\A_i$ and corresponding colour factors $C_{ij}$~\cite{Paton:1969je,Mangano:1987xk,DelDuca:1999rs,Maltoni:2002mq}, as denoted by the second equal sign, in which the $i,j$ indices label colour orderings of the external QCD particles. The only terms contributing at leading-colour in this colour sum are those with identical colour order, $i=j$ and so the LC-truncated cross section becomes
\begin{align}\label{eq.LC2}
\sigma_{\rm LC}\simeq \int \sum_i \A_i C_{ii}\A^*_i
\dd\Phi = \sum_{i} C_{ii} \int |\A_i|^2 \dd\Phi.
\end{align}
The two-step event generation proposed in Ref.~\cite{Frederix:2024uvy} starts from generating unweighted events using this LC approximation in the first step, treating each (unique) element in the sum as a separate integration channel. In the second step, these LC events are passed through a reweighting algorithm, in which each event is multiplied with the reweight factor
\begin{equation}\label{eq.rwfactor}
r^{\LC\to\FC}=\frac{|\M|^2}{\sum_{i} C_{ii}
  |\A_i|^2},
\end{equation}
which is the correction factor for obtaining the FC weight. This correction factor is evaluated using the specific kinematics and helicity of each event, in this way obtaining a FC-accurate (weighted) event sample. Finally, a secondary unweighting is performed in order to obtain equal-weight events.

The main result of Ref.~\cite{Frederix:2024uvy} is that this two-step procedure results in efficient event generation for all-QCD (i.e., multi-jet) processes, because the single elements in the sum of eq.~\ref{eq.LC2} are simple enough that the phase-space parametrisation introduced in Ref.~\cite{Byckling:1969luw} captures the structure of the integrand well if the building blocks are ordered according to the colour ordering of $|\A_i|^2$. This results in competative primary unweighting efficiencies in the LC event generation. Moreover, the reweight factor defined in eq.~\ref{eq.rwfactor} is close to a constant resulting in secondary unweighting efficiencies in the LC$\to$FC reweighting typically well-above 50\%.

For this work, we have extended the \AmpliCol code developed for Ref.~\cite{Frederix:2024uvy} to allow for any tree-level process generation in the Standard Model (SM), and focus on presenting results for the runtime to generate $10^5$ unweighted events (at full-colour accuracy) for standard-candle SM processes relevant to the LHC. 


\subsection{Colour singlets}

The main obstacle in the extension of the \AmpliCol program from QCD-only processes to the complete SM is the treatment of colour singlets in the generation of the LC events, i.e., the first step of the two-step procedure. The reason is that the order in which the $2\to3$ phase-space blocks introduced in Ref.~\cite{Byckling:1969luw} should be in a one-to-one correspondence with a single colour ordering of the LC amplitudes. In that way, the element of the sum of eq.~\ref{eq.LC2} can be treated as completely independent channels for which the $2\to3$ phase-space blocks form an efficient phase-space parameteriation, as shown in Ref.~\cite{Frederix:2024uvy}. Colour singlets do not directly fit into this picture: a LC approximation does not single out a unique ``ordering'' for colour singlets when there are multiple quark lines and/or multiple colour singlets. Hence, a phase-space parameterisation should be used that takes the possible orderings for the colour singlets into account. For this we adopt a multi-channel approach with weight optimisation, following Ref.~\cite{Kleiss:1994qy}, among all the channels that differ only by the unique positions of the colour singlets in the phase-space ordering.

Similarly to the colour-singlets, for processes with more than two quark lines there is also no longer a unique mapping between the colour-ordering and the the ordering used in the phase-space generation. This is due to the fact that interchanging two quark lines is no longer equivalent to cyclicly permuting the colour ordering beyond two-quark-line processes. While we also have implemented a multi-channel procedure to deal with three-quark-line processes, in practice our implementation is not very efficient: including the three-quark line processes seriously deteriorates the results presented below. In the light that contributions with three or more quark lines are subdominant in the computation of cross sections, we refrain from including them, and leave optimising the \AmpliCol code for these type of processes for future work. 


%\subsection{Phase-space integration}

%For the standard integration of a generic partonic process
%\begin{equation}
%a b \rightarrow 1 2 \ldots n
%\end{equation}
%we use the phase-space parametrisation introduced in Ref.~\cite{Byckling:1969luw} and used in Ref.~\cite{Frederix:2024uvy}. This parametrisation is based on a $2\rightarrow 3$ building block of consecutive momenta generations, following the colour ordering of the particles and maps all relavant variables of the maximally-helicity-violating (MHV) amplitudes~\cite{Parke:1986gb} to integration variables. The phase space integration is split to generate two sets of particles: those between particle $a$ and $b$, and those between $b$ and $a$. The particle kinematics in the two sets are then generated in a way that follows the peak structure of the MHV amplitudes, and therefore also for helicity summed amplitudes, and hence results in an efficient phase-space sampling.

%For the full LHC process including hadrons and jets, all possible subprocesses (treating the $b$-quark in the requested flavour-scheme) contributing to the requested collision are grouped in common phase-space order blocks. The given colour order of each block is then passed the phase-space generation described in the above, generating the kinematics in the specified order for the particles. The kinematics is then assigned for the different subprocesses in the block. With this method, one reduces overflow of phase-space samplings, improving further the efficiency of the algorithm.

%\subsection{Matrix-elements and subprocesses}

%The dual amplitudes are computed using the off-shell recursion relations, including all particles in the Standard Model. The main contributions to the partonic processes are, whenever present, the all-gluon processes. The subprocesses with quark lines are subdominant, and decrease in dominancy as the number of quark lines grow. Hence, we include the two-quark-line processes as standard to all the hadronic processes, however, the three-quark-line subprocesses are not included in the standard computation. We do, however, an analysis on their impact on the computation time, by including them on the level of the integration (generation of LC events). 

%\TV{Mention color singlet multi-channeling?}

%\TV{Mention the decomposition of SF processes?}



\section{Results}\label{sec:results}

We use the above method for the event generation of four benchmark LHC processes at center-of-mass energy of $\sqrt{s} = 14$ TeV: 
\begin{eqnarray}
\begin{split}
p p &\rightarrow nj,\\
pp &\rightarrow t\overline{t}+(n-2)j, \\
pp &\rightarrow ZZ+(n-2)j, \\
pp &\rightarrow e^+e^-+(n-1)j,
\end{split}
\end{eqnarray}
with the multiplicity number $n$ varied in the range $n \in [2,6]$ (for the multi-jet process up to $n=7$). In all processes we use a 5-flavour scheme and set the following cuts on the jets, whenever applicable:
\begin{eqnarray}
p_T(j) > 30 \text{ GeV} \quad,\quad \eta(j) < 6.0 \quad,\quad \Delta R(j_1,j_2) > 0.4.
\end{eqnarray}
In the Drell-Yan process, a cut on the leptonic invariant mass of $m(e^+e^-)>50$ GeV is used in order to avoid the photon-mediated singularity, and no further cuts on the leptons are placed. We set the renoralisation and factorisation scales equal to the $Z$ boson mass, and use the NNPDF23nlo set~\cite{Ball:2012cx} for the initial state parton distributions. \RF{Results for single PS points are cross-checked against madgraph where possible.} We perform all the computations on a single core of an Intel i7-8700K CPU running at 3.70GHz and extract the evaluation time, focussing on how the runtime scales with the multiplicity number $n$.


%\subsection{Runtime analysis}

The total time $t_{\rm tot}$ for the generation of unweighted events at FC accuracy in the two-step approach consists of
\begin{eqnarray}
t_{\rm tot} = t_{\rm int} + t_{\rm rwgt}\quad .
\end{eqnarray}
Here, the integration time $t_{\rm int}$ is for the generation of unweighted LC accurate events. This includes the setup of the integration grids, finding the upper bound of the event weights, and the primary unweighting of the generated events. At most a fraction of 0.1\% of the cross section, the so-called overweight fraction (see e.g.~\cite{Bothmann:2025lwg}), is truncated to improve the unweighting efficiency, but typically this number is between 0.05 to 0.07\% for the results presented in this work. The time for reweighting $t_{\rm rwgt}$ (plus secondary unweighting) is required to adjust the weights such that they capture the FC corrections. One must take into consideration the loss of events in the secondary unweighting step and generate the LC event number accordingly, such that the final number of FC events is the required event sample. We consider this fractional loss in the timings of the total time. In the following we compare these various times for the different processes.


\begin{figure}[htb!]
\hspace*{1cm}
\includegraphics[width=\textwidth]{results/timing_plot.pdf}
\caption{Computational time (in seconds) for the four benchmark processes. Shown is the total computation time (blob with error bar) for $10^5$ unweighted events at FC accuracy (including the generation time of the LC events, the reweighting time, and accounting for the secondary unweighting loss), and separately the reweighting time (triangle). An exponential curve is fitted to the total timings and portrayed as a line in the plots, with the base $b$ of the exponential indicated in each of the four processes.\RF{Fix label for e+e- process: $(n-1)$ instead of $(n-2)$} }
\label{fig:timings}
\end{figure}

In Fig.~\ref{fig:timings} we show the main results of this work: the time to generate $N=10^5$ unweighted events at FC accuracy on a single CPU core. It is shown for the four sample benchmark processes for varying jet multiplicity. The plot includes the total time $t_{\rm tot}$ required to generate these events (blobs with error bars) and also separately the reweighting time $t_{\rm rwgt}$ for each of the processes (triangles). In addition, a fit of an exponential curve is made to the total time with respect to the multiplicity $n$. The fit indicates an exponential growth for all processes with a base of around 5 (with a slight variance between process types, each shown in the figure). 

It is remarkable that an exponential fit covers the total timings resuts so well. Given that the contribution to the timing from the reweighting is always subdominant -- even for $pp \rightarrow 7j$ it is only about a third of the total time, we can conclude that the timing is dominated by the generation of events at LC accuracy. At LC the complexity of the matrix elements scales polynomially ($\sim n^3$) within a given integration channel and for a single helicity configuration, which means that it scales like $\sim 2^n$ due to the fact that \AmpliCol performs an explicit sum over the helicity configurations for each phase-space point. The remaining factor $2.5^n$-$3^n$ comes from the phase-space integration and unweighting: we have explicitly checked that in the \AmpliCol code the matrix elements need to be evaluated about 2.5-3 times more often in order to generate the same number of LC unweighted events when one increases the multiplicity by one. There is also a small increase in the runtime with multiplicity due to the decrease of the secondary unweighting efficiency, i.e.~in the LC$\to$FC reweighting, but this effect is subdominant because it stays approximately within the 60-80\% range as soon as there are at least 4 coloured particles in the process, see appendix~\ref{appendix}.

In the current implementation the reweight time $t_{\rm rwgt}$ scales faster-than-exponentially with particle multiplicity, as can be clearly seen from the triangles in Fig.~\ref{fig:timings}. Therefore, for large $n$, the time needed for reweighting will become the dominant part of the runtime. From the results in the figure, we expect this to happen when the number of coloured particles in the process (initial plus final state) is equal to ten. If matrix element calculations are needed for such large multiplicities, improvements to the reweighter part of \AmpliCol code would be very beneficial. One possibility would be to reweight to next-to-leading colour only~\cite{Frederix:2021wdv,Frederix:2024uvy}; but also ideas along the line of machine learning the reweight factors~\cite{Villadamigo:2025our} and using other colour bases to prevent the beyond-exponential growth at FC~\cite{Bolinder:2025gbj} are being investigated. 

%In the upper plots of Fig.~\ref{fig:3qq} \TV{Update values!} we show the absolute timing for generating $N=10^5$ events for the two QCD-only benchmark processes, the pure multi-jet process and top-quark-pair associated multi-jet process, comparing the time for the computation including three-quark-line subprocesses (darker dots) and the computation timing without the inclusion of these subprocesses. In the lower plots of Fig.~\ref{fig:3qq} \TV{Update values!} we verify the already known results that the cross section contribution from these subprocesses is on the percent level, indicating that the contribution of these subprocesses have smaller impact than the perturbtive expansion precision and hence could be omitted without obtaining results outside of the systematic errors of the computation. For this reason, these three-quark-line processes are generally not included in the reweighting step.


%\begin{figure}[htb!]
%\begin{center}
%\includegraphics[width=0.7\textwidth]{results/3qq_plot.pdf}
%\caption{Timing of the LC event generation for $10^5$ events for the two pure-QCD benchmark processes, with (darker markers) and without (lighter markers) including the three-quark-line subprocesses (upper plots). The ratio of the cross section for the two processes with  (left) and without (right) the three-quark-line processes (lower). \TV{Add legend}}
%\label{fig:3qq}
%\end{center}
%\end{figure}

%\subsection{Secondary unweighting efficiency}




\section{Discussion and outlook}\label{sec:outlook}

In this work we presented the program \AmpliCol, which performs the two-step event generation of high-multiplicity tree-level processes, with an exponential increase in computation time (at least up to moderately high multiplicities) rather than the factorial growth with conventional event generators. We illustrated the computation time for four benchmark processes with various multiplicities. We have shown that with the \AmpliCol code it is possible to generate leading order events for $2\rightarrow n$ processes for $n$ up to $7$ (for $pp\rightarrow \textrm{jets}$, $pp \rightarrow t\bar{t}+\textrm{jets}$ and $pp\rightarrow ZZ+\textrm{jets}$), or $8$ (for $pp\rightarrow e^+e^-+\textrm{jets}$) on a single CPU core with reasonable speed---for the most complicated of these it takes 1-2 days to generate $10^5$ unweighted events. The program will in the near future be interfaced with the \MG user-friendly matrix-element generator~\cite{Alwall:2014hca}, improving the automated computation of multi-jet processes, uncovering multiplicities which are currently out of reach within that framework. Moreover, the \AmpliCol code has a relatively simple structure, and the adaptation to run on multiple cores, or a GPU, should be relatively straight-forward, which could improve the wall-time for event generation significantly, if need be.

For all the processes presented in this work, the flavour combinations that include three (or more) quark lines have not been included. The reason is not that the code is not capable of including these sub-processes, but rather that the LC event generation efficiency deteriorates significantly when these are included, resulting in much longer runtime. It is expected that these contributions are more complicated than sub-processes with at most two quark lines: due to various possible orderings of the quark lines at LC, they are no longer in one-to-one correspondence (up to cyclic permutations) with a single, unique ordering used for phase-space generation. We have tried including a simple multi-channel approach, similar to what worked for colour singlets where the same problem arises, but without great success. We leave improving the efficiency for processes with multiple quark lines for future work. 

For multiplicities beyond the ones presented in this work, the reweight time to improve the accuracy from LC to FC, will become the dominant part of the runtime. In that case using techniques beyond what is currently used in \AmpliCol will be beneficial. Possibilities that are already being investigated are using Machine Learning to approximate the LC$\to$FC reweight factors~\cite{Villadamigo:2025our}, and using multiplet bases to tame the factorial growth in the computation of the FC matrix elements~\cite{Bolinder:2025gbj}.

Finally, we expect that efficiency gains observed when using machine learning to improve phase-space sampling, for example within the \textsc{MadNIS} framework~\cite{Heimel:2022wyj,Heimel:2023ngj,Heimel:2024wph}, can also be ap in the \AmpliCol code. 

%The current version of the program works at tree-level in the perutrbative expansion, and we leave the extension of the code for NLO computations for future work. 



\section*{Acknowledgements}

The work of R.F.~is supported by the Swedish Research Council under
project number 202004423.  The work of T.V.~is
supported by the Swedish Research Council under project number
VR:2023-00221. \TV{Update if needed}


\appendix

\section{Secondary unweighting efficiency}
\label{appendix}

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=\textwidth]{results/ess_plot.pdf}
\caption{Effective sample size $f_{\text{ESS}}$ for increasing jet multiplicity (left) and the unweighting efficiency $u^{\rm eff}$ (right) for the four benchmark processes considered. \RF{CHECK: is orange really ttbar, and green ZZ? and red e+e-?} \RF{Fix label for e+e- process: $(n-1)$ instead of $(n-2)$} }
\label{fig:ess}
\end{center}
\end{figure}

In our secondary unweighting procedure, i.e.~in the reweighting of the LC events to FC, we follow the usual procedure of first finding the maximum weight among a sample of events $w_{\rm max}$ and then sampling from the pool of events with the acceptance probability 
\begin{eqnarray}\label{eq:unw_eff}
u^{\rm eff}_i = \frac{w_i}{w_{\rm max}},
\end{eqnarray}
where $w_i$ is given by the reweight factor, $r^{\LC\to\FC}$. The fraction of events retained in this procedure is the secondary unweighted efficiency\footnote{Contrary to the primary unweighting, we allow for no overweight fraction since we are dealing with much smaller samples of weighted events.}. In Fig.~\ref{fig:ess} (left) we plot the secondary unweighting efficiency for the processes considered. This efficiency remains above approximately 60\% for all processes and multiplicities, indicating on average a sample size of roughly 1.7 larger at LC accuracy to generate the required number of FC events.


%The critical parts to such an unweighting procedure is the sample size used for the sampling of the maximum weight, and how to treat the cases when an event weight exceeds the maximum weight. One possibility is to discard the events with larger event weights, which is a decent approximation when this fraction is not too large. Another possibility is to perform an updating of the maximum weight, or to use a neural network based approach for improving the efficiency~\cite{Danziger:2021eeg,Gao:2020zvv}. In this work, we opt for \TV{Fill in}

Even though the secondary unweighting efficiency is large, one may consider keeping the weighted events instead. A way of assessing the statistical power of a weighted sample of size $N$ with weights $w_i$ is by evaluating the Kish effective sample size (ESS), defined by
\begin{eqnarray}
f_{\rm ESS} = \RF{\frac{1}{N}?}\frac{\left(\sum_{i=1}^N w_i\right)^2}{\sum_{i=1}^N w_i^2}.
\end{eqnarray}
This measure evaluates the spread of the weights, reaching a value of 1 for a sample of unweighted events (equal weights). A few outliers from an otherwise almost equal-weight event sample do not impact this effective size greatly, while one outlier will greatly decrease the unweighting efficiency defined in eq.~\ref{eq:unw_eff}. We show the ESS values for the four benchmark processes for varying multiplicity $n$ in the left(\RF{right})-hand plot of Fig.~\ref{fig:ess}. Even for the highest multiplicities at $n=6$, all of these processes remain at an ESS value of beyond 98\%, with the multi-jet processes scaling the best with the multiplicity increase. This suggests that keeping the weighted events and thereby reducing the effective sample size by less than 2\% significantly outperforms the unweighting the events, except in cases where post-processing (parton shower, hadronisation, detector simulation, etc.) is particularly time consuming. 

%When considering event sampling and unweighting, it is important to note that one can make minor modifications to the unweighting procedure by either vetoing a certain fraction of events with weights extending beyond the maximum weight used for the unweighting, or doing an on-the-fly updating of the maximum weight. In both of these solutions, one should keep in mind that if the fraction of events which are outliers in the unweighting is small, then the procedure is in essence unaffected. Hence, the ESS might reveal information on how suitable the event sample is for doing an unweighting with such small modifications for obtaining a still reasonable result. 



\bibliography{Paper}{}
\bibliographystyle{spphys} 
\end{document}
